
import json
import os
import torch
import time
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer
)
from datasets import Dataset

#1. Káº¾T Ná»I DRIVE
if not os.path.exists('/content/drive'):
    print("Äang káº¿t ná»‘i Google Drive...")
    drive.mount('/content/drive')

#2. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
BASE_PATH = "/content/drive/MyDrive/PhoBERT_Medical_Project"

if not os.path.exists(BASE_PATH):
    os.makedirs(BASE_PATH)
    print(f"âœ… ThÆ° má»¥c lÃ m viá»‡c: {BASE_PATH}")

MODEL_NAME = "vinai/phobert-large" 
DATA_FILE = "merged_train_data.jsonl" 

OUTPUT_DIR = os.path.join(BASE_PATH, "phobert_large_final")
CHECKPOINT_DIR = os.path.join(BASE_PATH, "checkpoints")
MAX_LENGTH = 128     

BATCH_SIZE = 32       
GRAD_ACCUMULATION = 1 
LEARNING_RATE = 2e-5  
EPOCHS = 1           

def print_status(msg):
    print(f"\nğŸš€ {msg}")

def check_environment():
    print("="*50)
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"âœ… GPU: {gpu_name} | VRAM: {vram:.2f} GB")
        print("âœ… Cáº¥u hÃ¬nh: MAX SPEED (Len 128 - Batch 32).")
    else:
        print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y GPU! HÃ£y báº­t GPU trong Runtime settings.")
        exit()
    print("="*50)

try:
    from underthesea import word_tokenize
except ImportError:
    print("âš ï¸ Äang cÃ i underthesea...")
    os.system("pip install underthesea")
    from underthesea import word_tokenize

def segment_text(text):
    if not text: return ""
    try:
        return word_tokenize(text, format="text")
    except:
        return text

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def get_latest_checkpoint(checkpoint_dir):
    if not os.path.exists(checkpoint_dir):
        return None
    checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith("checkpoint")]
    if not checkpoints:
        return None
    return max(checkpoints, key=os.path.getctime)

def find_cache_file(base_path):
    possible_names = ["train_data_cached.pt", "train_data_cached_colab.pt", "cached_data.pt"]
    print(f"ğŸ“‚ Äang tÃ¬m Cache trong: {base_path}")
    for name in possible_names:
        full_path = os.path.join(base_path, name)
        if os.path.exists(full_path):
            return full_path
    return None

def main():
    check_environment()
    torch.backends.cudnn.benchmark = True 

    texts = []
    labels = []
    label_map = {"Sai": 0, "ÄÃºng": 1} 

    #3. Xá»¬ LÃ Dá»® LIá»†U
    found_cache = find_cache_file(BASE_PATH)

    if found_cache:
        print_status(f"âœ… TÃŒM THáº¤Y CACHE: {found_cache}")
        print("   -> Äang load dá»¯ liá»‡u (SiÃªu nhanh)...")
        cached_data = torch.load(found_cache)
        texts = cached_data['texts']
        labels = cached_data['labels']
    else:
        print_status("âš ï¸ KHÃ”NG TÃŒM THáº¤Y CACHE. Báº®T Äáº¦U Xá»¬ LÃ Gá»C...")
        file_path_to_read = DATA_FILE
        drive_data_path = os.path.join(BASE_PATH, DATA_FILE)
        
        if os.path.exists(drive_data_path):
            file_path_to_read = drive_data_path
        elif not os.path.exists(DATA_FILE):
             print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file jsonl. HÃ£y upload lÃªn Drive!")
             return

        try:
            with open(file_path_to_read, 'r', encoding='utf-8') as f:
                raw_lines = f.readlines()
            
            print(f"   Äang tÃ¡ch tá»« {len(raw_lines)} dÃ²ng...")
            count = 0
            for line in raw_lines:
                try:
                    record = json.loads(line)
                    q = str(record.get('question', '')).strip()
                    a = str(record.get('answer', '')).strip()
                    if q and a in label_map:
                        texts.append(segment_text(q))
                        labels.append(label_map[a])
                        count += 1
                        if count % 5000 == 0: print(f"   {count}...", end='\r')
                except: continue
            
            new_cache_path = os.path.join(BASE_PATH, "train_data_cached.pt")
            print(f"\nâœ… Xá»­ lÃ½ xong. LÆ°u Cache vÃ o: {new_cache_path}")
            torch.save({'texts': texts, 'labels': labels}, new_cache_path)
            
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")
            return

    if len(texts) == 0: return

    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.1, random_state=42, stratify=labels
    )
    
    train_dataset = Dataset.from_dict({"text": train_texts, "label": train_labels})
    val_dataset = Dataset.from_dict({"text": val_texts, "label": val_labels})

    print_status(f"LOAD MODEL: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, num_labels=2, id2label={0: "Sai", 1: "ÄÃºng"}, label2id={"Sai": 0, "ÄÃºng": 1}
    )

    def preprocess_function(examples):

        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=MAX_LENGTH)

    print("âš™ï¸ Äang Tokenize (MÃ£ hÃ³a dá»¯ liá»‡u)...")
    encoded_train = train_dataset.map(preprocess_function, batched=True)
    encoded_val = val_dataset.map(preprocess_function, batched=True)

    #4. Cáº¤U HÃŒNH TRAINER
    training_args = TrainingArguments(
        output_dir=CHECKPOINT_DIR, 
        learning_rate=LEARNING_RATE,
        
        per_device_train_batch_size=BATCH_SIZE, 
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUMULATION,
        
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        
        eval_strategy="steps",     
        eval_steps=1000,                
        save_strategy="steps",          
        save_steps=1000,           
        save_total_limit=1,       
        load_best_model_at_end=True,
        
        fp16=True,                 
        group_by_length=True,      
        dataloader_num_workers=4,  
        dataloader_pin_memory=True,
        logging_steps=100,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=encoded_train,
        eval_dataset=encoded_val,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    #5. Tá»° Äá»˜NG KHÃ”I PHá»¤C (RESUME)
    latest_ckpt = get_latest_checkpoint(CHECKPOINT_DIR)
    if latest_ckpt:
        print_status(f"âš ï¸ TÃŒM THáº¤Y CHECKPOINT: {latest_ckpt}")
        print("   -> Äang khÃ´i phá»¥c train (Resume)...")
        trainer.train(resume_from_checkpoint=latest_ckpt)
    else:
        print_status(f"Báº®T Äáº¦U TRAIN Tá»C Äá»˜ CAO (Dá»± kiáº¿n 2-3 tiáº¿ng)...")
        trainer.train()

    #6. LÆ¯U Káº¾T QUáº¢
    print_status(f"LÆ¯U MODEL VÃ€O DRIVE: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("âœ… XONG! Model Ä‘Ã£ an toÃ n trÃªn Drive.")

if __name__ == "__main__":
    main()