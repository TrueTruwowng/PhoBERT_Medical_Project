import os
import json
import time
import pandas as pd
from tqdm import tqdm
import google.generativeai as genai
from google.api_core import exceptions

# --- C·∫§U H√åNH ---
API_KEY = "AIzaSyDBAuG-dHHuI1M5iImr1NgM4QN5Ky4uaqE"

INPUT_PARQUET_FILE = "train2.parquet" 
OUTPUT_FILE = "train_dataset_pubmedqa.jsonl"

BATCH_SIZE = 50 

genai.configure(api_key=API_KEY)
MODEL_NAME = 'models/gemini-2.5-flash-preview-09-2025'

# --- 1. SYSTEM PROMPT ---
SYSTEM_PROMPT = """
B·∫°n l√† chuy√™n gia d·ªØ li·ªáu y t·∫ø. Nhi·ªám v·ª•: Chuy·ªÉn ƒë·ªïi danh s√°ch c√¢u h·ªèi (Ti·∫øng Anh) sang c√¢u kh·∫≥ng ƒë·ªãnh (Ti·∫øng Vi·ªát).

INPUT: JSON {id, question, label}.
OUTPUT: JSON {id, cau_hoi, dap_an}.

QUY T·∫ÆC:
1. D·ª±a v√†o 'label' ƒë·ªÉ vi·∫øt 'cau_hoi' (Ti·∫øng Vi·ªát):
   - yes -> Vi·∫øt c√¢u kh·∫≥ng ƒë·ªãnh ƒê√öNG th·ª±c t·∫ø. (ƒê√°p √°n: ƒê√∫ng)
   - no -> Vi·∫øt c√¢u kh·∫≥ng ƒë·ªãnh SAI th·ª±c t·∫ø. (ƒê√°p √°n: Sai)
2. 'cau_hoi' l√† c√¢u k·ªÉ.
3. Tr·∫£ v·ªÅ JSON thu·∫ßn.
"""

# --- 2. H√ÄM G·ªåI API (BATCH) ---
def process_batch(batch_items):
    model = genai.GenerativeModel(model_name=MODEL_NAME, system_instruction=SYSTEM_PROMPT)
    
    mini_batch_input = []
    for item in batch_items:
        # Trong file parquet, t√™n c·ªôt l√† 'pubid', 'question', 'final_decision'
        mini_batch_input.append({
            "id": str(item['pubid']),
            "question": item['question'],
            "label": item['final_decision']
        })

    user_prompt = json.dumps(mini_batch_input)

    for attempt in range(3):
        try:
            response = model.generate_content(
                user_prompt,
                generation_config={"response_mime_type": "application/json"},
                request_options={'timeout': 90}
            )
            return json.loads(response.text)

        except exceptions.ResourceExhausted:
            print(f"   ‚è≥ H·∫øt quota. ƒê·ª£i 60s...")
            time.sleep(60)
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói batch: {e}")
            time.sleep(5)
    return None

# --- 3. H√ÄM ƒê·ªåC FILE PARQUET (AUTO-DETECT PATH) ---
def load_parquet_data():
    # L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file code n√†y
    base_path = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(base_path, INPUT_PARQUET_FILE)
    
    print(f"üìñ ƒêang ƒë·ªçc file: {file_path}...")
    
    if not os.path.exists(file_path):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file '{INPUT_PARQUET_FILE}'.")
        print(f"üëâ H√£y ch·∫Øc ch·∫Øn file '{INPUT_PARQUET_FILE}' n·∫±m c√πng th∆∞ m·ª•c v·ªõi file code.")
        exit()
        
    try:
        df = pd.read_parquet(file_path)
        # L·ªçc ch·ªâ l·∫•y yes/no
        df = df[df['final_decision'].isin(['yes', 'no'])]
        print(f"‚úÖ ƒê√£ t·∫£i {len(df)} d√≤ng d·ªØ li·ªáu h·ª£p l·ªá.")
        return df.to_dict('records')
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file Parquet: {e}")
        exit()

# --- 4. CHECKPOINT ---
def get_processed_ids(filepath):
    base_path = os.path.dirname(os.path.abspath(__file__))
    full_path = os.path.join(base_path, filepath)
    
    processed = set()
    if os.path.exists(full_path):
        with open(full_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'original_id' in data: processed.add(data['original_id'])
                except: pass
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(processed)} c√¢u ƒë√£ xong.")
    return processed

# --- 5. MAIN ---
def run():
    base_path = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(base_path, OUTPUT_FILE)
    
    all_items = load_parquet_data()
    processed_ids = get_processed_ids(OUTPUT_FILE)
    
    todo_items = [item for item in all_items if str(item['pubid']) not in processed_ids]
    
    print(f"üî• S·∫µn s√†ng x·ª≠ l√Ω {len(todo_items)} m·∫´u (Batch Size: {BATCH_SIZE})...")
    
    with open(output_path, "a", encoding="utf-8") as f_out:
        for i in tqdm(range(0, len(todo_items), BATCH_SIZE), desc="ƒêang d·ªãch"):
            batch = todo_items[i : i + BATCH_SIZE]
            
            result_list = process_batch(batch)
            
            if result_list and isinstance(result_list, list):
                for res in result_list:
                    final_record = {
                        "cau_hoi": res.get("cau_hoi"),
                        "dap_an": res.get("dap_an"),
                        "nguon": "PubMedQA (Artificial)",
                        "original_id": str(res.get("id"))
                    }
                    json.dump(final_record, f_out, ensure_ascii=False)
                    f_out.write('\n')
                f_out.flush()
            
            time.sleep(5)

    print(f"\nüéâ HO√ÄN T·∫§T! File k·∫øt qu·∫£: {output_path}")

if __name__ == "__main__":
    run()