#SINH CAU HOI MEDLATEC
import json
import os
from tqdm import tqdm
from vllm import LLM, SamplingParams

# --- Configuration ---
MODEL_ID = "Qwen/Qwen2.5-32B-Instruct-AWQ"
INPUT_FILE = 'medlatec_benh.json'  # Your data file name
OUTPUT_FILE = 'train_dataset_medlatec.jsonl'

print(f"ğŸš€ Äang khá»Ÿi Ä‘á»™ng Engine vLLM vá»›i {MODEL_ID}...") # Retained print

# Initialize vLLM Engine
# gpu_memory_utilization=0.9: Use 90% of A100 VRAM
llm = LLM(
    model=MODEL_ID,
    quantization="awq",
    dtype="half",
    gpu_memory_utilization=0.9,
    max_model_len=4096,
    enforce_eager=True  # Add this line if encountering CUDA graph errors on Colab
)

# Text generation configuration
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=1024,
    stop=["<|im_end|>", "<|endoftext|>"]
)

print("âœ… Engine sáºµn sÃ ng! Chuáº©n bá»‹ Ä‘ua tá»‘c Ä‘á»™...") # Retained print


def parse_sections(text):
    """Parses text into structured sections based on Vietnamese keywords."""
    if not text: return {}

    # Mapping output keys to Vietnamese keywords
    keywords = {
        "Tong_quan": ["Tá»•ng quan", "Äá»‹nh nghÄ©a", "lÃ  gÃ¬"],
        "Nguyen_nhan": ["NguyÃªn nhÃ¢n", "CÄƒn nguyÃªn"],
        "Trieu_chung": ["Triá»‡u chá»©ng", "Dáº¥u hiá»‡u"],
        "Chan_doan": ["Cháº©n Ä‘oÃ¡n", "XÃ©t nghiá»‡m"],
        "Dieu_tri": ["Äiá»u trá»‹", "Chá»¯a trá»‹", "Thuá»‘c"],
        "Phong_ngua": ["PhÃ²ng ngá»«a", "Dá»± phÃ²ng"],
        "Bien_chung": ["Biáº¿n chá»©ng", "Háº­u quáº£"]
    }

    sections = {}
    lines = text.split('\n')
    current_key = "Tong_quan"
    current_content = []

    for line in lines:
        line = line.strip()
        if not line: continue

        is_header = False
        for key, variants in keywords.items():
            # Check if line contains a section header keyword (case-insensitive)
            if any(v.lower() in line.lower() for v in variants) and len(line) < 100:
                if current_content:
                    # Store content of the previous section
                    sections[current_key] = "\n".join(current_content)

                # Start new section
                current_key = key
                current_content = []
                is_header = True
                break

        if not is_header:
            current_content.append(line)

    # Store the content of the last section
    if current_content: sections[current_key] = "\n".join(current_content)

    return sections

def make_prompt(disease_name, category, context):
    """Creates the Qwen Chat standard prompt for question generation."""
    return f"""<|im_start|>system
Báº¡n lÃ  chuyÃªn gia y táº¿. Táº¡o bá»™ cÃ¢u há»i tráº¯c nghiá»‡m ÄÃºng/Sai.<|im_end|>
<|im_start|>user
Bá»‡nh: {disease_name}
Má»¥c: {category}
Ná»™i dung: "{context[:3000]}"

YÃŠU Cáº¦U:
1. Sinh 4-6 cÃ¢u nháº­n Ä‘á»‹nh (Statement).
2. 50% cÃ¢u ÄÃšNG, 50% cÃ¢u SAI.
3. Input lÃ  cÃ¢u KHáº²NG Äá»ŠNH. KHÃ”NG viáº¿t cÃ¢u há»i.
4. Chá»‰ tráº£ vá» JSON List: [{{"input": "...", "output": "..."}}]<|im_end|>
<|im_start|>assistant
"""

def main():
    """Main function to load data, generate prompts, run inference, and save results."""
    if not os.path.exists(INPUT_FILE):
        print("KhÃ´ng tÃ¬m tháº¥y file input!") # Retained print
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        if isinstance(data, dict): data = [data]

    # --- STEP 1: CREATE LIST OF ALL PROMPTS ---
    all_prompts = []
    metadata = []  # Stores accompanying info to map results back

    print("ğŸ”„ Äang chuáº©n bá»‹ dá»¯ liá»‡u...") # Retained print
    for item in tqdm(data, desc="Parsing"):
        clean_text = item.get('clean_text', '')
        url = item.get('url', '')
        # Extract disease name, handling potential "Tá»•ng quan" prefix
        disease_name = clean_text.split('\n')[0].replace("Tá»•ng quan", "").strip() or "Bá»‡nh lÃ½"

        sections = parse_sections(clean_text)

        for sec_key, sec_content in sections.items():
            if len(sec_content) < 50: continue # Skip very short sections

            prompt = make_prompt(disease_name, sec_key, sec_content)
            all_prompts.append(prompt)
            metadata.append({"url": url, "category": sec_key})

    print(f"ğŸ“¦ Tá»•ng cá»™ng cÃ³ {len(all_prompts)} tÃ¡c vá»¥ cáº§n xá»­ lÃ½.") # Retained print

    # --- STEP 2: RUN BATCH INFERENCE ---
    print("ğŸš€ Báº®T Äáº¦U CHáº Y BATCH TRÃŠN A100...") # Retained print
    # This part is much faster than traditional iterative generation
    outputs = llm.generate(all_prompts, sampling_params)

    # --- STEP 3: SAVE RESULTS ---
    count = 0
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        for i, output in enumerate(outputs):
            generated_text = output.outputs[0].text
            meta = metadata[i]

            try:
                # Manual JSON parsing/extraction (robust to extra text)
                start = generated_text.find('[')
                end = generated_text.rfind(']') + 1

                if start != -1 and end != -1:
                    # Load the list of Q&A dictionaries
                    qa_list = json.loads(generated_text[start:end])

                    for qa in qa_list:
                        inp = qa.get('input', '')
                        # Basic filtering for quality
                        if len(inp) < 15 or "?" in inp: continue

                        final_obj = {
                            # Instruction generation based on category
                            "instruction": f"Dá»±a vÃ o kiáº¿n thá»©c y há»c vá» {meta['category']}, hÃ£y xÃ¡c Ä‘á»‹nh nháº­n Ä‘á»‹nh sau lÃ  ÄÃºng hay Sai.",
                            "input": inp,
                            "output": qa.get('output', ''),
                            "source": meta['url'],
                            "category": meta['category']
                        }

                        # Write the final JSON object in jsonl format
                        json.dump(final_obj, f_out, ensure_ascii=False)
                        f_out.write('\n')
                        count += 1
            except:
                # Skip if JSON parsing fails
                continue

    print(f"\n HOÃ€N Táº¤T! ÄÃ£ sinh Ä‘Æ°á»£c {count} cÃ¢u há»i.") # Retained print
    print(f"ğŸ“‚ Káº¿t quáº£ lÆ°u táº¡i: {OUTPUT_FILE}") # Retained print

if __name__ == "__main__":
    main()