import requests
from bs4 import BeautifulSoup
import time
import random
import os
import string # Th∆∞ vi·ªán ƒë·ªÉ l·∫•y b·∫£ng ch·ªØ c√°i a-z

# --- C·∫§U H√åNH THEO H√åNH ·∫¢NH ---
# URL g·ªëc ch∆∞a c√≥ ch·ªØ c√°i. Ch√∫ng ta s·∫Ω c·ªông chu·ªói "a", "b", "c" v√†o sau.
BASE_URL_PREFIX = "https://www.vinmec.com/vie/tra-cuu-benh/"

# File l∆∞u k·∫øt qu·∫£
current_folder = os.path.dirname(os.path.abspath(__file__))
OUTPUT_FILE = os.path.join(current_folder, "vinmec_links_az.txt")

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def crawl_by_alphabet():
    print("üöÄ B·∫ÆT ƒê·∫¶U CHI·∫æN D·ªäCH C√ÄO THEO B·∫¢NG CH·ªÆ C√ÅI A-Z")
    print(f"üìÇ K·∫øt qu·∫£ l∆∞u t·∫°i: {OUTPUT_FILE}")
    print("-" * 60)

    total_links = 0
    
    # T·∫°o danh s√°ch ch·ªØ c√°i: ['a', 'b', 'c', ..., 'z']
    alphabet_list = list(string.ascii_lowercase) 
    
    # M·ªü file ch·∫ø ƒë·ªô 'a' (append)
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        
        # V√íNG L·∫∂P T·ª™ A ƒê·∫æN Z
        for char in alphabet_list:
            # T·∫°o link: https://www.vinmec.com/vie/tra-cuu-benh/a
            url = f"{BASE_URL_PREFIX}{char}"
            print(f"\nüì° ƒêang qu√©t ch·ªØ c√°i [{char.upper()}]: {url}")
            
            try:
                response = requests.get(url, headers=HEADERS, timeout=15)
                
                # N·∫øu ch·ªØ c√°i ƒë√≥ kh√¥ng c√≥ b·ªánh (v√≠ d·ª• ch·ªØ X, Y c√≥ th·ªÉ √≠t), Vinmec v·∫´n tr·∫£ v·ªÅ 200
                if response.status_code != 200:
                    print(f"‚ö†Ô∏è L·ªói truy c·∫≠p ch·ªØ {char}. B·ªè qua.")
                    continue

                soup = BeautifulSoup(response.content, 'lxml')
                
                # --- B√ìC T√ÅCH LINK ---
                # D·ª±a v√†o ·∫£nh b·∫°n g·ª≠i, danh s√°ch b·ªánh n·∫±m d∆∞·ªõi ch·ªØ c√°i A to t∆∞·ªõng
                # Th∆∞·ªùng Vinmec ƒë·ªÉ link b·ªánh trong th·∫ª <li> -> <a> ho·∫∑c <h2> -> <a>
                
                # C√°ch an to√†n nh·∫•t: L·∫•y t·∫•t c·∫£ th·∫ª <a> trong v√πng n·ªôi dung ch√≠nh
                # V√† l·ªçc nh·ªØng link ch·ª©a '/vie/benh/' ho·∫∑c '/vi/benh/'
                
                all_links = soup.find_all('a')
                
                count_char = 0
                for tag in all_links:
                    raw_link = tag.get('href')
                    
                    if raw_link:
                        # 1. Chu·∫©n h√≥a link
                        if not raw_link.startswith('http'):
                            full_link = "https://www.vinmec.com" + raw_link
                        else:
                            full_link = raw_link
                        
                        # 2. B·ªò L·ªåC (Quan tr·ªçng)
                        # Link b·ªánh trong ·∫£nh 1 b·∫°n g·ª≠i c√≥ d·∫°ng: .../vie/benh/addison...
                        # V√¨ v·∫≠y ta ch·ªâ l·∫•y link ch·ª©a '/benh/'
                        # V√† lo·∫°i b·ªè ch√≠nh c√°i link trang danh m·ª•c (/tra-cuu-benh/)
                        if '/benh/' in full_link and '/tra-cuu-benh/' not in full_link:
                            
                            # L∆∞u v√†o file
                            f.write(full_link + '\n')
                            
                            # In ra v√†i c√°i ƒë·ªÉ ki·ªÉm tra (kh√¥ng in h·∫øt cho ƒë·ª° r·ªëi m·∫Øt)
                            if count_char < 3: 
                                print(f"   + {full_link}")
                            elif count_char == 3:
                                print("   + ... (v√† c√°c b√†i kh√°c)")
                                
                            count_char += 1
                            total_links += 1

                if count_char == 0:
                    print(f"‚ùå Kh√¥ng t√¨m th·∫•y b·ªánh n√†o b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ {char.upper()}")
                else:
                    print(f"‚úÖ Ch·ªØ {char.upper()}: L·∫•y ƒë∆∞·ª£c {count_char} b·ªánh.")

                # Ng·ªß ƒë·ªÉ tr√°nh ch·∫∑n (quan tr·ªçng v√¨ v√≤ng l·∫∑p a-z ch·∫°y kh√° nhanh)
                time.sleep(random.uniform(1.5, 3))

            except Exception as e:
                print(f"‚ùå L·ªói t·∫°i ch·ªØ {char}: {e}")

    print("-" * 60)
    print(f"üéâ HO√ÄN T·∫§T A-Z! T·ªïng c·ªông: {total_links} ƒë∆∞·ªùng link.")

if __name__ == "__main__":
    crawl_by_alphabet()