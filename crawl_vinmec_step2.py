import requests
from bs4 import BeautifulSoup
import time
import random
import json
import os
import re

# --- Cáº¤U HÃŒNH ---
# Äá»c file link Ä‘Ã£ cÃ o á»Ÿ BÆ°á»›c 1
INPUT_FILE = "vinmec_links_az.txt"
# TÃªn file káº¿t quáº£ má»›i, sáº¡ch sáº½
OUTPUT_FILE = "vinmec_data_step2_final.jsonl" 

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# --- HÃ€M 1: Gá»¬I REQUEST KIÃŠN TRÃŒ ---
def get_with_retry(url, max_retries=3):
    """Cá»‘ gáº¯ng truy cáº­p URL. Náº¿u lá»—i máº¡ng/timeout, Ä‘á»£i vÃ  thá»­ láº¡i."""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            
            # Kiá»ƒm tra soft ban (cháº·n IP)
            if "Access Denied" in response.text or "Verify" in response.text:
                print(f"   ğŸš¨ Cáº¢NH BÃO: Bá»‹ cháº·n IP. Äang Ä‘á»£i 30 giÃ¢y...")
                time.sleep(30)
                continue # Thá»­ láº¡i sau khi Ä‘á»£i
                
            if response.status_code == 200:
                return response
            elif response.status_code in [403, 429, 503]:
                print(f"   âš ï¸ Server báº­n (Code {response.status_code}). Äá»£i 10s...")
                time.sleep(10)
            else:
                return None # Lá»—i 404...
                
        except requests.exceptions.RequestException as e:
            wait = (attempt + 1) * 5
            print(f"   âš ï¸ Lá»—i máº¡ng (Láº§n {attempt+1}): {e}. Äá»£i {wait}s...")
            time.sleep(wait)
            
    print(f"   âŒ Bá» QUA: KhÃ´ng thá»ƒ truy cáº­p {url} sau {max_retries} láº§n.")
    return None

# --- HÃ€M 2: CHUáº¨N HÃ“A TIÃŠU Äá»€ ---
def normalize_header(text):
    """Gom nhÃ³m cÃ¡c tiÃªu Ä‘á» vá» 8 má»¥c chÃ­nh"""
    text = text.lower()
    if any(x in text for x in ["tá»•ng quan", "lÃ  gÃ¬", "lÃ  bá»‡nh gÃ¬"]): return "Tá»•ng quan"
    if "nguyÃªn nhÃ¢n" in text: return "NguyÃªn nhÃ¢n"
    if any(x in text for x in ["triá»‡u chá»©ng", "dáº¥u hiá»‡u", "biá»ƒu hiá»‡n"]): return "Triá»‡u chá»©ng"
    if any(x in text for x in ["lÃ¢y truyá»n", "lÃ¢y lan", "Ä‘Æ°á»ng lÃ¢y"]): return "ÄÆ°á»ng lÃ¢y truyá»n"
    if any(x in text for x in ["Ä‘á»‘i tÆ°á»£ng", "nguy cÆ¡", "ai máº¯c"]): return "Äá»‘i tÆ°á»£ng nguy cÆ¡"
    if any(x in text for x in ["phÃ²ng ngá»«a", "phÃ²ng bá»‡nh", "cháº¿ Ä‘á»™ sinh hoáº¡t"]): return "PhÃ²ng ngá»«a"
    if any(x in text for x in ["cháº©n Ä‘oÃ¡n", "xÃ©t nghiá»‡m"]): return "Biá»‡n phÃ¡p cháº©n Ä‘oÃ¡n"
    if any(x in text for x in ["Ä‘iá»u trá»‹", "chá»¯a trá»‹", "thuá»‘c"]): return "Biá»‡n phÃ¡p Ä‘iá»u trá»‹"
    return "ThÃ´ng tin khÃ¡c"

# --- HÃ€M 3: BÃ“C TÃCH THÃ”NG MINH (Báº£n sá»­a lá»—i) ---
def parse_details_smart(url):
    """HÃ m nÃ y lÃ  trÃ¡i tim cá»§a code, bÃ³c tÃ¡ch ná»™i dung."""
    response = get_with_retry(url)
    if not response: return []
    
    try:
        soup = BeautifulSoup(response.content, 'lxml')
        
        # --- 1. Sá»¬A Lá»–I UNKNOWN TÃŠN Bá»†NH ---
        benh_name = "Unknown"
        h1 = soup.find('h1')
        if h1 and len(h1.text.strip()) > 2:
            benh_name = h1.text.strip()
        else:
            # Láº¥y dá»± phÃ²ng tá»« tháº» <title>
            title_tag = soup.find('title')
            if title_tag:
                raw_title = title_tag.text
                parts = re.split(r'[:|\-]', raw_title) # Cáº¯t tÃªn bá»‡nh trÆ°á»›c dáº¥u : hoáº·c -
                if parts:
                    benh_name = parts[0].strip()

        # --- 2. Sá»¬A Lá»–I 0 BÃ€I VIáº¾T (TÃ¬m khung ná»™i dung) ---
        content_div = None
        keywords = ["NguyÃªn nhÃ¢n", "Triá»‡u chá»©ng", "Äiá»u trá»‹", "Tá»•ng quan"]
        
        # Chiáº¿n thuáº­t 1: TÃ¬m theo tá»« khÃ³a (Æ¯u tiÃªn)
        for kw in keywords:
            target_header = soup.find(lambda tag: tag.name in ['h2', 'h3'] and kw in tag.get_text())
            if target_header:
                content_div = target_header.parent
                if content_div and len(content_div.get_text(strip=True)) < 200 and content_div.parent:
                    content_div = content_div.parent # Leo lÃªn 1 cáº¥p náº¿u khung quÃ¡ nhá»
                break
        
        # Chiáº¿n thuáº­t 2: Náº¿u khÃ´ng tháº¥y tá»« khÃ³a, tÃ¬m theo class (Dá»± phÃ²ng)
        if not content_div:
             classes = ['collapsible-content', 'main-content', 'post-content', 'body-content']
             for cls in classes:
                 div = soup.find('div', class_=cls)
                 if div and len(div.get_text(strip=True)) > 200:
                     content_div = div
                     break

        if not content_div:
            print(f"   âŒ KhÃ´ng tÃ¬m tháº¥y khung ná»™i dung nÃ o.")
            return [] # Bá» qua bÃ i nÃ y

        # --- 3. BÃ“C TÃCH Dá»® LIá»†U ---
        extracted_data = []
        current_category = "Tá»•ng quan"
        current_content = []
        
        for tag in content_div.find_all(['h2', 'h3', 'p', 'ul']): # Chá»‰ láº¥y 4 tháº» quan trá»ng nÃ y
            text = tag.get_text(separator=" ").strip()
            if not text: continue
            
            if tag.name in ['h2', 'h3']:
                # Gáº·p tiÃªu Ä‘á» má»›i -> LÆ°u Ä‘oáº¡n cÅ©
                if current_content:
                    full_text = " ".join(current_content).strip()
                    if len(full_text) > 20:
                        extracted_data.append({
                            "benh": benh_name,
                            "muc": current_category,
                            "noi_dung": full_text
                        })
                current_category = normalize_header(text)
                current_content = []
            
            elif tag.name in ['p', 'ul']:
                # Lá»c rÃ¡c quáº£ng cÃ¡o
                if any(s in text for s in ["Vinmec", "Äáº¶T Lá»ŠCH", "Táº I ÄÃ‚Y", "Xem thÃªm"]): 
                    continue
                current_content.append(text)

        # LÆ°u Ä‘oáº¡n cuá»‘i cÃ¹ng
        if current_content:
            extracted_data.append({
                "benh": benh_name,
                "muc": current_category,
                "noi_dung": " ".join(current_content).strip()
            })
            
        return extracted_data

    except Exception as e:
        print(f"   âŒ Lá»—i xá»­ lÃ½ HTML: {e}")
        return []

# --- HÃ€M CHÃNH (MAIN) ---
def run_crawler():
    base_path = os.path.dirname(os.path.abspath(__file__))
    input_path = os.path.join(base_path, INPUT_FILE)
    output_path = os.path.join(base_path, OUTPUT_FILE)
    
    if not os.path.exists(input_path):
        print(f"âš ï¸ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file '{input_path}'! HÃ£y cháº¡y BÆ°á»›c 1 trÆ°á»›c.")
        return

    with open(input_path, 'r', encoding='utf-8') as f:
        # Lá»c bá» cÃ¡c link rÃ¡c (náº¿u cÃ³)
        urls = [line.strip() for line in f.readlines() if line.strip() and "/benh/" in line]

    total_links = len(urls)
    total_chunks = 0
    print(f"ğŸš€ Báº¯t Ä‘áº§u cÃ o ná»™i dung tá»« {total_links} link (Báº£n sá»­a lá»—i)...")
    print(f"ğŸ“‚ Dá»¯ liá»‡u sáº½ lÆ°u vÃ o: {output_path}")
    
    # Má»Ÿ file cháº¿ Ä‘á»™ 'a' (append)
    with open(output_path, 'a', encoding='utf-8') as f_out:
        for i, url in enumerate(urls):
            print(f"[{i+1}/{total_links}] Äang cÃ o: {url}")
            
            chunks = parse_details_smart(url)
            
            if chunks:
                # In tÃªn bá»‡nh Ä‘á»ƒ kiá»ƒm tra
                print(f"   âœ… Láº¥y Ä‘Æ°á»£c {len(chunks)} Ä‘oáº¡n. (Bá»‡nh: {chunks[0]['benh']})")
                
                for chunk in chunks:
                    json.dump(chunk, f_out, ensure_ascii=False)
                    f_out.write('\n')
                total_chunks += len(chunks)
                
                # --- Sá»¬A Lá»–I FILE Rá»–NG ---
                # Ã‰p Python xáº£ dá»¯ liá»‡u tá»« RAM ra á»• cá»©ng ngay láº­p tá»©c
                f_out.flush() 
            
            # Nghá»‰ ngÆ¡i Ä‘á»ƒ khÃ´ng bá»‹ cháº·n
            time.sleep(random.uniform(1.5, 3))

    print("-" * 50)
    print(f"ğŸ‰ HOÃ€N Táº¤T!")
    print(f"   - Sá»‘ bÃ i viáº¿t Ä‘Ã£ xá»­ lÃ½ thÃ nh cÃ´ng: {total_links} (trá»« cÃ¡c link lá»—i)")
    print(f"   - Tá»•ng sá»‘ máº©u dá»¯ liá»‡u (chunks) thu Ä‘Æ°á»£c: {total_chunks}")
    print(f"ğŸ‘‰ Kiá»ƒm tra file káº¿t quáº£: {output_path}")

if __name__ == "__main__":
    run_crawler()